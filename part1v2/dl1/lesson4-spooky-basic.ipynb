{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/wgilliam/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# machine learning\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer,  TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "\n",
    "# nlp\n",
    "from spacy.en.language_data import STOP_WORDS\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "from spooky import *\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/spooky'\n",
    "\n",
    "os.makedirs(f'{PATH}/models', exist_ok=True)\n",
    "os.makedirs(f'{PATH}/tmp', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 19579 | Test size: 8392\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get raw training and test datasets\n",
    "train_df = pd.read_csv(f'{PATH}/train.csv')\n",
    "test_df = pd.read_csv(f'{PATH}/test.csv')\n",
    "sample_subm_df = pd.read_csv(f'{PATH}/sample_submission.csv')\n",
    "\n",
    "print(f'Training size: {len(train_df)} | Test size: {len(test_df)}')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add representations of the text with stopwords removed (cleaned), lemmatized, and a lemmatized version with stopwords removed.\n",
    "\n",
    "Also add columns, that for each document, represent the percentage of various parts of speech and/or the existence of various named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "    rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        cols = OrderedDict()\n",
    "        \n",
    "        # grab tokens, entities, and word tokens\n",
    "        tokens = nlp(row['text'])\n",
    "        ents = tokens.ents\n",
    "        words = [ token for token in tokens if (not token.is_punct) ]\n",
    "        \n",
    "        # add different versions of text\n",
    "        cols['cleaned_text'] = ' '.join([ t.text for t in tokens if (not t.is_stop) ])\n",
    "        cols['lemmatized_text'] = ' '.join([ t.lemma_ for t in tokens ])\n",
    "        cols['cleaned_lemmatized_text'] = ' '.join([ t.lemma_ for t in tokens if (not t.is_stop) ])\n",
    "        \n",
    "        # character and word counts\n",
    "        cols['char_count'] = len(row['text'])\n",
    "        cols['word_count'] = len(words)\n",
    "        \n",
    "        # ratio of token types to words\n",
    "        cols['u_word_pct'] = len(set([ w.lemma_ for w in words ])) / len(words)\n",
    "        cols['stopwords_pct'] = len([ w for w in words if (w.is_stop) ]) / len(words)\n",
    "        cols['punctuation_pct'] = len([ t for t in tokens if (t.is_punct) ]) / len(tokens)\n",
    "        cols['symbol_pct'] = len([ t for t in tokens if (t.pos_ == 'SYM') ]) / len(words)\n",
    "        cols['number_pct'] = len([ t for t in tokens if (t.pos_ == 'NUM') ]) / len(words)\n",
    "        cols['alpha_pct'] = len([ t for t in tokens if (t.is_alpha) ]) / len(words)\n",
    "        \n",
    "        cols['noun_pct'] = len([ t for t in tokens if (t.pos_ == 'NOUN') ]) / len(words)\n",
    "        cols['verb_pct'] = len([ t for t in tokens if (t.pos_ == 'VERB') ]) / len(words)\n",
    "        cols['adj_pct'] = len([ t for t in tokens if (t.pos_ == 'ADJ') ]) / len(words)\n",
    "        cols['proper_name_pct'] = len([ t for t in tokens if (t.pos_ == 'PROPN') ]) / len(words)\n",
    "    \n",
    "        # ratio of named entity types\n",
    "        cols['named_entity_pct'] = len(ents) / len(words)\n",
    "        cols['named_entity_person_pct'] = len([ ent for ent in ents if (ent.label_ == 'PERSON') ]) / len(words)\n",
    "        cols['named_entity_norp_pct'] = len([ ent for ent in ents if (ent.label_ == 'NORP') ]) / len(words)\n",
    "        cols['named_entity_facility_pct'] = len([ ent for ent in ents if (ent.label_ == 'FACILITY') ]) / len(words)\n",
    "        cols['named_entity_org_pct'] = len([ ent for ent in ents if (ent.label_ == 'ORG') ]) / len(words)\n",
    "        cols['named_entity_gpe_pct'] = len([ ent for ent in ents if (ent.label_ == 'GPE') ]) / len(words)\n",
    "        cols['named_entity_non_gpe_loc_pct'] = len([ ent for ent in ents if (ent.label_ == 'LOC') ]) / len(words)\n",
    "        cols['named_entity_product_pct'] = len([ ent for ent in ents if (ent.label_ == 'PRODUCT') ]) / len(words)\n",
    "        cols['named_entity_event_pct'] = len([ ent for ent in ents if (ent.label_ == 'EVENT') ]) / len(words)\n",
    "        cols['named_entity_woa_pct'] = len([ ent for ent in ents if (ent.label_ == 'WORK_OF_ART') ]) / len(words)\n",
    "        cols['named_entity_lang_pct'] = len([ ent for ent in ents if (ent.label_ == 'LANGUAGE') ]) / len(words)\n",
    "        cols['named_entity_date_pct'] = len([ ent for ent in ents if (ent.label_ == 'DATE') ]) / len(words)\n",
    "        cols['named_entity_time_pct'] = len([ ent for ent in ents if (ent.label_ == 'TIME') ]) / len(words)\n",
    "        cols['named_entity_money_pct'] = len([ ent for ent in ents if (ent.label_ == 'MONEY') ]) / len(words)\n",
    "        cols['named_entity_quantity_pct'] = len([ ent for ent in ents if (ent.label_ == 'QUANTITY') ]) / len(words)\n",
    "\n",
    "        rows.append(cols)\n",
    "        \n",
    "    return pd.DataFrame(rows, columns=cols.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, add_cols(train_df)], axis=1)\n",
    "test_df = pd.concat([test_df, add_cols(test_df)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add probabilites and predictions learned from sentiment analysis with pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 6) (8392, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP_probs</th>\n",
       "      <th>MWS_probs</th>\n",
       "      <th>HPL_probs</th>\n",
       "      <th>EAP_preds</th>\n",
       "      <th>MWS_preds</th>\n",
       "      <th>HPL_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.963837</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.032615</td>\n",
       "      <td>3.197491</td>\n",
       "      <td>-2.407068</td>\n",
       "      <td>-0.188668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.431109</td>\n",
       "      <td>0.030999</td>\n",
       "      <td>0.537893</td>\n",
       "      <td>1.043664</td>\n",
       "      <td>-1.588753</td>\n",
       "      <td>1.264962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972993</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>3.097029</td>\n",
       "      <td>-1.006164</td>\n",
       "      <td>-1.391549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.989481</td>\n",
       "      <td>0.009452</td>\n",
       "      <td>-2.368515</td>\n",
       "      <td>4.463643</td>\n",
       "      <td>-0.187351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.155847</td>\n",
       "      <td>0.046061</td>\n",
       "      <td>0.798092</td>\n",
       "      <td>0.258638</td>\n",
       "      <td>-0.960270</td>\n",
       "      <td>1.891985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EAP_probs  MWS_probs  HPL_probs  EAP_preds  MWS_preds  HPL_preds\n",
       "0   0.963837   0.003548   0.032615   3.197491  -2.407068  -0.188668\n",
       "1   0.431109   0.030999   0.537893   1.043664  -1.588753   1.264962\n",
       "2   0.972993   0.016074   0.010933   3.097029  -1.006164  -1.391549\n",
       "3   0.001067   0.989481   0.009452  -2.368515   4.463643  -0.187351\n",
       "4   0.155847   0.046061   0.798092   0.258638  -0.960270   1.891985"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lm_results_df = pd.read_csv(f'{PATH}/20171207_2_train_preds.csv')\n",
    "test_lm_results_df = pd.read_csv(f'{PATH}/20171207_2_test_preds.csv')\n",
    "\n",
    "train_lm_results_df.drop(columns=['text', 'id', 'author'], axis=1, inplace=True)\n",
    "test_lm_results_df.drop(columns=['text', 'id'], axis=1, inplace=True)\n",
    "\n",
    "print(train_lm_results_df.shape, test_lm_results_df.shape)\n",
    "train_lm_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_lemmatized_text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>u_word_pct</th>\n",
       "      <th>stopwords_pct</th>\n",
       "      <th>punctuation_pct</th>\n",
       "      <th>symbol_pct</th>\n",
       "      <th>number_pct</th>\n",
       "      <th>alpha_pct</th>\n",
       "      <th>noun_pct</th>\n",
       "      <th>verb_pct</th>\n",
       "      <th>adj_pct</th>\n",
       "      <th>proper_name_pct</th>\n",
       "      <th>named_entity_pct</th>\n",
       "      <th>named_entity_person_pct</th>\n",
       "      <th>named_entity_norp_pct</th>\n",
       "      <th>named_entity_facility_pct</th>\n",
       "      <th>named_entity_org_pct</th>\n",
       "      <th>named_entity_gpe_pct</th>\n",
       "      <th>named_entity_non_gpe_loc_pct</th>\n",
       "      <th>named_entity_product_pct</th>\n",
       "      <th>named_entity_event_pct</th>\n",
       "      <th>named_entity_woa_pct</th>\n",
       "      <th>named_entity_lang_pct</th>\n",
       "      <th>named_entity_date_pct</th>\n",
       "      <th>named_entity_time_pct</th>\n",
       "      <th>named_entity_money_pct</th>\n",
       "      <th>named_entity_quantity_pct</th>\n",
       "      <th>EAP_probs</th>\n",
       "      <th>MWS_probs</th>\n",
       "      <th>HPL_probs</th>\n",
       "      <th>EAP_preds</th>\n",
       "      <th>MWS_preds</th>\n",
       "      <th>HPL_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>process , , afforded means ascertaining dimens...</td>\n",
       "      <td>this process , however , afford -PRON- no mean...</td>\n",
       "      <td>process , , afford means ascertain dimension d...</td>\n",
       "      <td>231</td>\n",
       "      <td>41</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.963837</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.032615</td>\n",
       "      <td>3.197491</td>\n",
       "      <td>-2.407068</td>\n",
       "      <td>-0.188668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>occurred fumbling mere mistake .</td>\n",
       "      <td>-PRON- never once occur to -PRON- that the fum...</td>\n",
       "      <td>occur fumbling mere mistake .</td>\n",
       "      <td>71</td>\n",
       "      <td>14</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.431109</td>\n",
       "      <td>0.030999</td>\n",
       "      <td>0.537893</td>\n",
       "      <td>1.043664</td>\n",
       "      <td>-1.588753</td>\n",
       "      <td>1.264962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>left hand gold snuff box , , capered hill , cu...</td>\n",
       "      <td>in -PRON- left hand be a gold snuff box , from...</td>\n",
       "      <td>left hand gold snuff box , , caper hill , cut ...</td>\n",
       "      <td>200</td>\n",
       "      <td>36</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972993</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>3.097029</td>\n",
       "      <td>-1.006164</td>\n",
       "      <td>-1.391549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>lovely spring looked Windsor Terrace sixteen f...</td>\n",
       "      <td>how lovely be spring as -PRON- look from winds...</td>\n",
       "      <td>lovely spring look windsor terrace sixteen fer...</td>\n",
       "      <td>206</td>\n",
       "      <td>34</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.989481</td>\n",
       "      <td>0.009452</td>\n",
       "      <td>-2.368515</td>\n",
       "      <td>4.463643</td>\n",
       "      <td>-0.187351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>Finding , gold , Superintendent abandoned atte...</td>\n",
       "      <td>find nothing else , not even gold , the superi...</td>\n",
       "      <td>find , gold , superintendent abandon attempt ;...</td>\n",
       "      <td>174</td>\n",
       "      <td>27</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155847</td>\n",
       "      <td>0.046061</td>\n",
       "      <td>0.798092</td>\n",
       "      <td>0.258638</td>\n",
       "      <td>-0.960270</td>\n",
       "      <td>1.891985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  process , , afforded means ascertaining dimens...   \n",
       "1                   occurred fumbling mere mistake .   \n",
       "2  left hand gold snuff box , , capered hill , cu...   \n",
       "3  lovely spring looked Windsor Terrace sixteen f...   \n",
       "4  Finding , gold , Superintendent abandoned atte...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  this process , however , afford -PRON- no mean...   \n",
       "1  -PRON- never once occur to -PRON- that the fum...   \n",
       "2  in -PRON- left hand be a gold snuff box , from...   \n",
       "3  how lovely be spring as -PRON- look from winds...   \n",
       "4  find nothing else , not even gold , the superi...   \n",
       "\n",
       "                             cleaned_lemmatized_text  char_count  word_count  \\\n",
       "0  process , , afford means ascertain dimension d...         231          41   \n",
       "1                      occur fumbling mere mistake .          71          14   \n",
       "2  left hand gold snuff box , , caper hill , cut ...         200          36   \n",
       "3  lovely spring look windsor terrace sixteen fer...         206          34   \n",
       "4  find , gold , superintendent abandon attempt ;...         174          27   \n",
       "\n",
       "   u_word_pct  stopwords_pct  punctuation_pct  symbol_pct  number_pct  \\\n",
       "0    0.780488       0.634146         0.145833         0.0    0.000000   \n",
       "1    0.928571       0.714286         0.066667         0.0    0.000000   \n",
       "2    0.861111       0.472222         0.121951         0.0    0.000000   \n",
       "3    0.911765       0.411765         0.105263         0.0    0.029412   \n",
       "4    0.888889       0.518519         0.129032         0.0    0.000000   \n",
       "\n",
       "   alpha_pct  noun_pct  verb_pct   adj_pct  proper_name_pct  named_entity_pct  \\\n",
       "0        1.0  0.219512  0.195122  0.097561         0.000000          0.000000   \n",
       "1        1.0  0.142857  0.214286  0.071429         0.000000          0.000000   \n",
       "2        1.0  0.305556  0.111111  0.166667         0.000000          0.000000   \n",
       "3        1.0  0.205882  0.147059  0.176471         0.058824          0.058824   \n",
       "4        1.0  0.222222  0.185185  0.148148         0.037037          0.000000   \n",
       "\n",
       "   named_entity_person_pct  named_entity_norp_pct  named_entity_facility_pct  \\\n",
       "0                      0.0                    0.0                        0.0   \n",
       "1                      0.0                    0.0                        0.0   \n",
       "2                      0.0                    0.0                        0.0   \n",
       "3                      0.0                    0.0                        0.0   \n",
       "4                      0.0                    0.0                        0.0   \n",
       "\n",
       "   named_entity_org_pct  named_entity_gpe_pct  named_entity_non_gpe_loc_pct  \\\n",
       "0                   0.0              0.000000                           0.0   \n",
       "1                   0.0              0.000000                           0.0   \n",
       "2                   0.0              0.000000                           0.0   \n",
       "3                   0.0              0.029412                           0.0   \n",
       "4                   0.0              0.000000                           0.0   \n",
       "\n",
       "   named_entity_product_pct  named_entity_event_pct  named_entity_woa_pct  \\\n",
       "0                       0.0                     0.0                   0.0   \n",
       "1                       0.0                     0.0                   0.0   \n",
       "2                       0.0                     0.0                   0.0   \n",
       "3                       0.0                     0.0                   0.0   \n",
       "4                       0.0                     0.0                   0.0   \n",
       "\n",
       "   named_entity_lang_pct  named_entity_date_pct  named_entity_time_pct  \\\n",
       "0                    0.0                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.0                    0.0                    0.0   \n",
       "3                    0.0                    0.0                    0.0   \n",
       "4                    0.0                    0.0                    0.0   \n",
       "\n",
       "   named_entity_money_pct  named_entity_quantity_pct  EAP_probs  MWS_probs  \\\n",
       "0                     0.0                        0.0   0.963837   0.003548   \n",
       "1                     0.0                        0.0   0.431109   0.030999   \n",
       "2                     0.0                        0.0   0.972993   0.016074   \n",
       "3                     0.0                        0.0   0.001067   0.989481   \n",
       "4                     0.0                        0.0   0.155847   0.046061   \n",
       "\n",
       "   HPL_probs  EAP_preds  MWS_preds  HPL_preds  \n",
       "0   0.032615   3.197491  -2.407068  -0.188668  \n",
       "1   0.537893   1.043664  -1.588753   1.264962  \n",
       "2   0.010933   3.097029  -1.006164  -1.391549  \n",
       "3   0.009452  -2.368515   4.463643  -0.187351  \n",
       "4   0.798092   0.258638  -0.960270   1.891985  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train_df, train_lm_results_df], axis=1)\n",
    "test_df = pd.concat([test_df, test_lm_results_df], axis=1)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0].text\n",
    "# train_df[train_df.named_entity_person_pct > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# train_df['cleaned_text'] = train_df.text.apply(\n",
    "#     lambda txt: ' '.join([ word.text for word in nlp(txt) if (not word.is_stop) ]))\n",
    "\n",
    "# train_df['lemmatized_text'] = train_df.text.apply(\n",
    "#     lambda txt: ' '.join([ word.lemma_ for word in nlp(txt) ]))\n",
    "\n",
    "# train_df['cleaned_lemmatized_text'] = train_df.cleaned_text.apply(\n",
    "#     lambda txt: ' '.join([ word.lemma_ for word in nlp(txt) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # counts\n",
    "# def char_count(row):\n",
    "#     return len(row['text'])\n",
    "\n",
    "# def word_count(row):\n",
    "#     tokens = [ token.text for token in nlp(row['text']) if (not token.is_punct) ]\n",
    "#     return len(tokens)\n",
    "\n",
    "# def u_word_pct(row):\n",
    "#     tokens = [ token.lemma_ for token in nlp(row['text']) if (not token.is_punct) ]\n",
    "#     return len(list(set(tokens))) / row['word_count']\n",
    "\n",
    "# # ratio of token types\n",
    "# def stopwords_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.is_stop) ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def punctuation_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.is_punct) ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def noun_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'NOUN') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def verb_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'VERB') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def adj_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'ADJ') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def proper_name_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'PROPN') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def symbol_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'SYM') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def number_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'NUM') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def alpha_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.is_alpha) ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# # ratio of named entity types and specific named entities\n",
    "# def named_entity_pct(row):\n",
    "#     return len(nlp(row['text']).ents) / row['word_count']\n",
    "\n",
    "# def named_entity_person_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'PERSON')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_norp_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'NORP')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_facility_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'FACILITY')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_org_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'ORG')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_gpe_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'GPE')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_non_gpe_loc_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'LOC')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_product_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'PRODUCT')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_event_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'EVENT')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_woa_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'WORK_OF_ART')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_lang_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'LANGUAGE')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_date_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'DATE')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_time_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'TIME')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_money_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'MONEY')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_quantity_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'QUANTITY')]\n",
    "#     return len(ents) / row['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # char and word counts\n",
    "# train_df['char_count'] = train_df.apply(lambda r: char_count(r), axis=1)\n",
    "# train_df['word_count'] = train_df.apply(lambda r: word_count(r), axis=1)\n",
    "\n",
    "# # unqique, stop, and punctuation percentages\n",
    "# train_df['u_word_pct'] = train_df.apply(lambda r: u_word_pct(r), axis=1)\n",
    "# train_df['stopwords_pct'] = train_df.apply(lambda r: stopwords_pct(r), axis=1)\n",
    "# train_df['punctuation_pct'] = train_df.apply(lambda r: punctuation_pct(r), axis=1)\n",
    "\n",
    "# # noun, verb, adj, proper name percentages\n",
    "# train_df['noun_pct'] = train_df.apply(lambda r: noun_pct(r), axis=1)\n",
    "# train_df['verb_pct'] = train_df.apply(lambda r: verb_pct(r), axis=1)\n",
    "# train_df['adj_pct'] = train_df.apply(lambda r: adj_pct(r), axis=1)\n",
    "# train_df['proper_name_pct'] = train_df.apply(lambda r: proper_name_pct(r), axis=1)\n",
    "\n",
    "# # alpha, number, symbol percentages\n",
    "# train_df['alpha_pct'] = train_df.apply(lambda r: alpha_pct(r), axis=1)\n",
    "# train_df['number_pct'] = train_df.apply(lambda r: number_pct(r), axis=1)\n",
    "# train_df['symbol_pct'] = train_df.apply(lambda r: symbol_pct(r), axis=1)\n",
    "\n",
    "# # named entity percentages\n",
    "# train_df['named_entity_pct'] = train_df.apply(lambda r: named_entity_pct(r), axis=1)\n",
    "# train_df['named_entity_person_pct'] = train_df.apply(lambda r: named_entity_person_pct(r), axis=1)\n",
    "# train_df['named_entity_norp_pct'] = train_df.apply(lambda r: named_entity_norp_pct(r), axis=1)\n",
    "# train_df['named_entity_facility_pct'] = train_df.apply(lambda r: named_entity_facility_pct(r), axis=1)\n",
    "# train_df['named_entity_org_pct'] = train_df.apply(lambda r: named_entity_org_pct(r), axis=1)\n",
    "# train_df['named_entity_gpe_pct'] = train_df.apply(lambda r: named_entity_gpe_pct(r), axis=1)\n",
    "# train_df['named_entity_non_gpe_loc_pct'] = train_df.apply(lambda r: named_entity_non_gpe_loc_pct(r), axis=1)\n",
    "# train_df['named_entity_product_pct'] = train_df.apply(lambda r: named_entity_product_pct(r), axis=1)\n",
    "# train_df['named_entity_event_pct'] = train_df.apply(lambda r: named_entity_event_pct(r), axis=1)\n",
    "# train_df['named_entity_woa_pct'] = train_df.apply(lambda r: named_entity_woa_pct(r), axis=1)\n",
    "# train_df['named_entity_lang_pct'] = train_df.apply(lambda r: named_entity_lang_pct(r), axis=1)\n",
    "# train_df['named_entity_date_pct'] = train_df.apply(lambda r: named_entity_date_pct(r), axis=1)\n",
    "# train_df['named_entity_time_pct'] = train_df.apply(lambda r: named_entity_time_pct(r), axis=1)\n",
    "# train_df['named_entity_money_pct'] = train_df.apply(lambda r: named_entity_money_pct(r), axis=1)\n",
    "# train_df['named_entity_quantity_pct'] = train_df.apply(lambda r: named_entity_quantity_pct(r), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(f'{PATH}/train_fe.csv', index=None)\n",
    "test_df.to_csv(f'{PATH}/test_fe.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{PATH}/train_fe.csv', index_col=None)\n",
    "test_df = pd.read_csv(f'{PATH}/test_fe.csv', index_col = None)\n",
    "\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our multi-class logloss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Models\n",
    "\n",
    "Let's try fitting several models on the cleaned up lemmatized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode lables\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train_df.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df.cleaned_lemmatized_text.values, \n",
    "                                                      y, stratify=y, test_size=0.2, \n",
    "                                                      random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15663,), (3916,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer** class to count how many times each term shows up in each document\n",
    "\n",
    "Parameters:\n",
    "- **min_df** (min. document frequency): The minimum number of documents a term must be present in (integer or if float, represents the percentage of documents)\n",
    "\n",
    "- **max_df** (max. document frequency): The maximum number of documents a term can be found in (int or float, see above)\n",
    "\n",
    "Words that are too infrequent or too frequent lack predictive power.\n",
    "\n",
    "See: http://www.ultravioletanalytics.com/2016/11/18/tf-idf-basics-with-pandas-scikit-learn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378726"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), stop_words=STOP_WORDS)\n",
    "# ctv\n",
    "\n",
    "ctv.fit(list(X_train) + list(X_valid))\n",
    "X_train_ctv = ctv.transform(X_train)\n",
    "X_valid_ctv = ctv.transform(X_valid)\n",
    "\n",
    "len(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seek', 292876),\n",
       " ('return', 278539),\n",
       " ('tenebrous', 333451),\n",
       " ('labyrinth', 181871),\n",
       " ('direct', 85152)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(ctv.vocabulary_.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the document into a **“bag of words”** representation which essentially is just a separate column for each term containing the count within each document. \n",
    "\n",
    "The **sparsity** of this representation which lets us know how many nonzero values there are in the dataset. The more sparse the data is the more challenging it will be to model, but that’s a discussion for another day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (19579, 378726)\n",
      "nonzero count: 601329\n",
      "sparsity: 0.01%\n"
     ]
    }
   ],
   "source": [
    "ctv_counts = ctv.transform(train_df.cleaned_lemmatized_text)\n",
    "print ('sparse matrix shape:', ctv_counts.shape)\n",
    "print ('nonzero count:', ctv_counts.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * ctv_counts.nnz / (ctv_counts.shape[0] * ctv_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occurrences</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>285168</th>\n",
       "      <td>1268</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202905</th>\n",
       "      <td>1135</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55154</th>\n",
       "      <td>880</td>\n",
       "      <td>come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340425</th>\n",
       "      <td>867</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179929</th>\n",
       "      <td>839</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122915</th>\n",
       "      <td>834</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73059</th>\n",
       "      <td>742</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111690</th>\n",
       "      <td>726</td>\n",
       "      <td>eye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335466</th>\n",
       "      <td>725</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143397</th>\n",
       "      <td>679</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        occurrences   term\n",
       "285168         1268      s\n",
       "202905         1135    man\n",
       "55154           880   come\n",
       "340425          867   time\n",
       "179929          839   know\n",
       "122915          834   find\n",
       "73059           742    day\n",
       "111690          726    eye\n",
       "335466          725  thing\n",
       "143397          679  great"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 most common words\n",
    "occ = np.asarray(ctv_counts.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': ctv.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve got term counts for each document we can use the TfidfTransformer to calculate the weights for each term in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf_weights = tfidf.fit_transform(ctv_counts)\n",
    "tfidf_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 terms by average tf-idf weight\n",
    "weights = np.asarray(tfidf_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': ctv.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Logistic Regression on word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train_ctv, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_ctv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Naive Bayes on word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_ctv, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_ctv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**TF-IDF (Term Frequency - Inverse Document Frequency)**: A technique for determining what each document, in a set of documents, is about.\n",
    "\n",
    "**TF** (term frequency) = The porportion of occurences of a specific term to the total number of terms in a document\n",
    "\n",
    "**IDF** (inverse document frequency) = The inverse of the porportion of documents that contain a specifc word/phrase\n",
    "\n",
    "*The general idea is that if a specific phrase appears a lot of times in a given document, but it doesn’t appear in many other documents, then we have a good idea that the phrase is important in distinguishing that document from all the others.*\n",
    "\n",
    "For each term, we will have a separate feature (e.g., if there are 10k terms we will have 10k features), the value will be the tf-idf weight of that term in the document.\n",
    "\n",
    "Note: You want to use stopwords, stemming/lemmatization *first* to narrow down your corpus to the *important* words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=None, \n",
    "            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1, sublinear_tf=1,\n",
    "            stop_words=STOP_WORDS)\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(X_train) + list(X_valid))\n",
    "X_train_tfv =  tfv.transform(X_train) \n",
    "X_valid_tfv = tfv.transform(X_valid)\n",
    "\n",
    "X_train_tfv.shape, X_valid_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# top 10 weights for training set\n",
    "weights = np.asarray(X_train_tfv.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': tfv.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fit simple logistic regression on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train_tfv, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_tfv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fit Naive Bayes on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfv, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_tfv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SVM\n",
    "\n",
    "Since SVMs take a lot of time, we will reduce the number of features from the TF-IDF using Singular Value Decomposition before applying SVM.\n",
    "\n",
    "Also, note that before applying SVMs, we must standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# apply SVD, I chose 120 components (120-200 components are good enough for SVM model)\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(X_train_tfv)\n",
    "X_train_svd = svd.transform(X_train_tfv)\n",
    "X_valid_svd = svd.transform(X_valid_tfv)\n",
    "\n",
    "# scale the data obtained from SVD ... renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(X_train_svd)\n",
    "X_train_svd_scl = scl.transform(X_train_svd)\n",
    "X_valid_svd_scl = scl.transform(X_valid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(X_train_svd_scl, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on word counts\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(X_train_ctv.tocsc(), y_train)\n",
    "preds = clf.predict_proba(X_valid_ctv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(X_train_tfv.tocsc(), y_train)\n",
    "preds = clf.predict_proba(X_valid_tfv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on SVD features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(X_train_svd, y_train)\n",
    "preds = clf.predict_proba(X_valid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting a VERY simple xgboost on SVD features\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "\n",
    "clf.fit(X_train_svd, y_train)\n",
    "preds = clf.predict_proba(X_valid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try using more of the engineered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27589\n",
      "(19579, 27589) (8392, 27589)\n"
     ]
    }
   ],
   "source": [
    "# fit CountVectorizer on ENTIRE training dataset\n",
    "ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), stop_words=STOP_WORDS, \n",
    "                      min_df=0.00006)\n",
    "\n",
    "ctv.fit(train_df.cleaned_lemmatized_text.values)\n",
    "train_ctv = ctv.transform(train_df.cleaned_lemmatized_text.values)\n",
    "test_ctv = ctv.transform(test_df.cleaned_lemmatized_text.values)\n",
    "\n",
    "print(len(ctv.vocabulary_))\n",
    "print(train_ctv.shape, test_ctv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'author', 'text', \n",
    "                'cleaned_text', 'lemmatized_text', 'cleaned_lemmatized_text',\n",
    "                'EAP_preds', 'MWS_preds', 'HPL_preds']\n",
    "\n",
    "train = train_df.drop(columns=cols_to_drop).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 27619)\n"
     ]
    }
   ],
   "source": [
    "# !!! NOTE: train_ctv will be a sparse array, so to concatenate you have to cast it to \".toarray()\n",
    "train = np.concatenate([train, train_ctv.toarray()], axis=1)\n",
    "print(train.shape)\n",
    "\n",
    "# define train and validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train, y, \n",
    "                                                      stratify=y, test_size=0.2, \n",
    "                                                      random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.402 \n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.316 \n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))\n",
    "print (clf.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))\n",
    "print (clf.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gausian Naive Bayes\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))\n",
    "print (clf.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'author', 'text', \n",
    "                'cleaned_text', 'lemmatized_text', 'cleaned_lemmatized_text',\n",
    "               'EAP_preds', 'MWS_preds', 'HPL_preds']\n",
    "\n",
    "train = train_df.drop(columns=cols_to_drop).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=0.00006, max_features=None, \n",
    "            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1,3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "            stop_words=STOP_WORDS)\n",
    "\n",
    "tfv.fit(train_df.cleaned_lemmatized_text.values)\n",
    "train_tfv = tfv.transform(train_df.cleaned_lemmatized_text.values)\n",
    "print(train_tfv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! NOTE: train_ctv will be a sparse array, so to concatenate you have to cast it to \".toarray()\n",
    "train = np.concatenate([train, train_tfv.toarray()], axis=1)\n",
    "print(train.shape)\n",
    "\n",
    "# define train and validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train, y, \n",
    "                                                      stratify=y, test_size=0.1, \n",
    "                                                      random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))\n",
    "print (clf.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))\n",
    "print (clf.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gausian Naive Bayes\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))\n",
    "print (clf.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "When feature engineering is done, we usually try to decrease the dimensionality by selecting the \"right\" number of features that captures the essentials and will allow our model to generalize better\n",
    "\n",
    "Benefits of feature selection:\n",
    "* It decreases redundancy among the data\n",
    "* It speeds up the training process\n",
    "* It reduces overfitting\n",
    "\n",
    "Tree-based estimators can be used to compute feature importances, which can in turn be used to discard irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'text', 'cleaned_text', 'lemmatized_text', 'cleaned_lemmatized_text',\n",
    "               'EAP_preds', 'MWS_preds', 'HPL_preds']\n",
    "\n",
    "train = train_df.drop(columns=cols_to_drop + ['author']).as_matrix()\n",
    "test = test_df.drop(columns=cols_to_drop).as_matrix()\n",
    "\n",
    "# !!! NOTE: train_ctv will be a sparse array, so to concatenate you have to cast it to \".toarray()\n",
    "X_train = np.concatenate([train, train_ctv.toarray()], axis=1)\n",
    "y_train = y\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=200)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['Feature'] = train_df.drop(columns=cols_to_drop+['author']).columns.tolist() + list(ctv.vocabulary_.keys())\n",
    "features['Importance'] = clf.feature_importances_\n",
    "features.index = features.Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sort_values('Importance', ascending=False, inplace=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:10].plot(kind='barh', figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now transform our training set and test to be more compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(clf, prefit=True, threshold=6e-04)\n",
    "\n",
    "X_train_new = model.transform(X_train)\n",
    "# X_test_new = model.transform(X_test)\n",
    "\n",
    "X_train_new.shape #, X_test_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning\n",
    "\n",
    "Random Forest come with a variety of parameters that you can tweak to get an optimal model for your prediction task.\n",
    "\n",
    "To learn more about Random Forest:  https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "multiclass_ll_scorer = make_scorer(multiclass_logloss, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is built by scanning all combinations of hyperparameters and selecting the best combination for predictions against a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "# parameter_grid = {\n",
    "#     'max_features': ['sqrt'],\n",
    "#     'max_depth' : [4, 5, 6, 7, 8],\n",
    "#     'n_estimators': [200, 210, 240, 250],\n",
    "#     'criterion': ['gini','entropy']\n",
    "# }\n",
    "\n",
    "parameter_grid = {\n",
    "    'max_depth': [4, 5, 6, 7, 8],\n",
    "    'n_estimators': [200, 210, 240, 250],\n",
    "    'max_features': ['sqrt', 'auto', 'log2'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [2, 5, 10],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "cross_validation = StratifiedKFold(y_train, n_folds=5)\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid=parameter_grid, cv=cross_validation)\n",
    "grid_search.fit(X_train_new, y_train)\n",
    "\n",
    "print(f'Best score: {grid_search.best_score_}')\n",
    "print(f'Best parameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_new, \n",
    "                                                      y, stratify=y, test_size=0.2, \n",
    "                                                      random_state=42, shuffle=True)\n",
    "\n",
    "preds = grid_search.predict_proba(X_valid)\n",
    "multiclass_logloss(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27589\n",
      "(19579, 27589) (8392, 27589)\n"
     ]
    }
   ],
   "source": [
    "# fit CountVectorizer on ENTIRE training dataset\n",
    "ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), stop_words=STOP_WORDS, \n",
    "                      min_df=0.00006)\n",
    "\n",
    "ctv.fit(train_df.cleaned_lemmatized_text.values)\n",
    "train_ctv = ctv.transform(train_df.cleaned_lemmatized_text.values)\n",
    "test_ctv = ctv.transform(test_df.cleaned_lemmatized_text.values)\n",
    "\n",
    "print(len(ctv.vocabulary_))\n",
    "print(train_ctv.shape, test_ctv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 27619) (19579,)\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = ['id', 'text', 'cleaned_text', 'lemmatized_text', 'cleaned_lemmatized_text',\n",
    "               'EAP_preds', 'MWS_preds', 'HPL_preds']\n",
    "\n",
    "train = train_df.drop(columns=cols_to_drop + ['author']).as_matrix()\n",
    "test = test_df.drop(columns=cols_to_drop).as_matrix()\n",
    "\n",
    "# !!! NOTE: train_ctv will be a sparse array, so to concatenate you have to cast it to \".toarray()\n",
    "X_train = np.concatenate([train, train_ctv.toarray()], axis=1)\n",
    "X_test = np.concatenate([test, test_ctv.toarray()], axis=1)\n",
    "y_train = y\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8835487001379029\n",
      "Best parameters: {'C': 2.0, 'max_iter': 100, 'tol': 0.0001, 'warm_start': True}\n",
      "CPU times: user 29min 1s, sys: 2min 41s, total: 31min 43s\n",
      "Wall time: 12min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', random_state=42)\n",
    "\n",
    "parameter_grid = {\n",
    "    'tol': [1e-4, 1e-5, 1e-6],\n",
    "    'C': [0.5, 1.0, 2.0],\n",
    "    #'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'max_iter': [100, 200, 400],\n",
    "    'warm_start': [True, False]\n",
    "}\n",
    "\n",
    "cross_validation = StratifiedKFold(y_train, n_folds=5)\n",
    "\n",
    "grid_search = GridSearchCV(clf, \n",
    "                           param_grid=parameter_grid, \n",
    "                           #scoring=multiclass_ll_scorer,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best score: {grid_search.best_score_}')\n",
    "print(f'Best parameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters: {'C': 2.0, 'max_iter': 100, 'tol': 1e-06, 'warm_start': True}\n",
    "preds = grid_search.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.005939</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.987117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.997527</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.001227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.040332</td>\n",
       "      <td>0.953727</td>\n",
       "      <td>0.005940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.993705</td>\n",
       "      <td>0.002391</td>\n",
       "      <td>0.003904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.847733</td>\n",
       "      <td>0.124917</td>\n",
       "      <td>0.027350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.005939  0.006944  0.987117\n",
       "1  id24541  0.997527  0.001246  0.001227\n",
       "2  id00134  0.040332  0.953727  0.005940\n",
       "3  id27757  0.993705  0.002391  0.003904\n",
       "4  id04081  0.847733  0.124917  0.027350"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_df = pd.DataFrame(preds, columns=lbl_enc.classes_)\n",
    "subm_df.insert(0, 'id', test_df.id)\n",
    "subm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df.to_csv(f'{PATH}/201711209_wg_lr+fe.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.868583686602993\n",
      "Best parameters: {'alpha': 1.0, 'fit_prior': True}\n",
      "CPU times: user 1min 20s, sys: 19.8 s, total: 1min 39s\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "parameter_grid = {\n",
    "    'alpha': [1.0, 2.0, 3.0],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "cross_validation = StratifiedKFold(y_train, n_folds=5)\n",
    "\n",
    "grid_search = GridSearchCV(clf, \n",
    "                           param_grid=parameter_grid, \n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best score: {grid_search.best_score_}')\n",
    "print(f'Best parameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>9.991003e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.252739e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.104171</td>\n",
       "      <td>0.895821</td>\n",
       "      <td>7.986799e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.950031</td>\n",
       "      <td>0.049969</td>\n",
       "      <td>2.391387e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.991003</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>2.060484e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL           MWS\n",
       "0  id02310  0.000792  0.000108  9.991003e-01\n",
       "1  id24541  0.999984  0.000003  1.252739e-05\n",
       "2  id00134  0.104171  0.895821  7.986799e-06\n",
       "3  id27757  0.950031  0.049969  2.391387e-08\n",
       "4  id04081  0.991003  0.006937  2.060484e-03"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = grid_search.predict_proba(X_test)\n",
    "\n",
    "subm_df = pd.DataFrame(preds, columns=lbl_enc.classes_)\n",
    "subm_df.insert(0, 'id', test_df.id)\n",
    "subm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df.to_csv(f'{PATH}/201711209_wg_2_lr+fe.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
