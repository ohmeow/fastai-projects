{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wgilliam/Development/_tools/anaconda/envs/fastai/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from spacy.en.language_data import STOP_WORDS\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,  TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from spooky import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/spooky'\n",
    "\n",
    "os.makedirs(f'{PATH}/models', exist_ok=True)\n",
    "os.makedirs(f'{PATH}/tmp', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 19579 | Test size: 8392\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get raw training and test datasets\n",
    "train_df = pd.read_csv(f'{PATH}/train.csv')\n",
    "test_df = pd.read_csv(f'{PATH}/test.csv')\n",
    "\n",
    "print(f'Training size: {len(train_df)} | Test size: {len(test_df)}')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add probabilites and predictions learned from sentiment analysis with pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lm_results_df = pd.re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new columns that represent the text fragments with stopwords removed, lemmatized, and only non-stopwords lemmatized\n",
    "\n",
    "For each document, add word count, % unique words, % stop words, % punctuation, % of nouns, % of adjectives, % of proper names, % of numbers, % of symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "    rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        cols = OrderedDict()\n",
    "        \n",
    "        # grab tokens, entities, and word tokens\n",
    "        tokens = nlp(row['text'])\n",
    "        ents = tokens.ents\n",
    "        words = [ token for token in tokens if (not token.is_punct) ]\n",
    "        \n",
    "        cols['cleaned_text'] = ' '.join([ t.text for t in tokens if (not t.is_stop) ])\n",
    "        cols['lemmatized_text'] = ' '.join([ t.lemma_ for t in tokens ])\n",
    "        cols['cleaned_lemmatized_text'] = ' '.join([ t.lemma_ for t in tokens if (not t.is_stop) ])\n",
    "        \n",
    "        # character and word counts\n",
    "        cols['char_count'] = len(row['text'])\n",
    "        cols['word_count'] = len(words)\n",
    "        \n",
    "        # ratio of token types to words\n",
    "        cols['u_word_pct'] = len(set([ w.lemma_ for w in words ])) / len(words)\n",
    "        cols['stopwords_pct'] = len([ w for w in words if (w.is_stop) ]) / len(words)\n",
    "        cols['punctuation_pct'] = len([ t for t in tokens if (t.is_punct) ]) / len(tokens)\n",
    "        cols['symbol_pct'] = len([ t for t in tokens if (t.pos_ == 'SYM') ]) / len(words)\n",
    "        cols['number_pct'] = len([ t for t in tokens if (t.pos_ == 'NUM') ]) / len(words)\n",
    "        cols['alpha_pct'] = len([ t for t in tokens if (t.is_alpha) ]) / len(words)\n",
    "        \n",
    "        cols['noun_pct'] = len([ t for t in tokens if (t.pos_ == 'NOUN') ]) / len(words)\n",
    "        cols['verb_pct'] = len([ t for t in tokens if (t.pos_ == 'VERB') ]) / len(words)\n",
    "        cols['adj_pct'] = len([ t for t in tokens if (t.pos_ == 'ADJ') ]) / len(words)\n",
    "        cols['proper_name_pct'] = len([ t for t in tokens if (t.pos_ == 'PROPN') ]) / len(words)\n",
    "    \n",
    "        # ratio of named entity types\n",
    "        cols['named_entity_pct'] = len(ents) / len(words)\n",
    "        cols['named_entity_person_pct'] = len([ ent for ent in ents if (ent.label_ == 'PERSON') ]) / len(words)\n",
    "        cols['named_entity_norp_pct'] = len([ ent for ent in ents if (ent.label_ == 'NORP') ]) / len(words)\n",
    "        cols['named_entity_facility_pct'] = len([ ent for ent in ents if (ent.label_ == 'FACILITY') ]) / len(words)\n",
    "        cols['named_entity_org_pct'] = len([ ent for ent in ents if (ent.label_ == 'ORG') ]) / len(words)\n",
    "        cols['named_entity_gpe_pct'] = len([ ent for ent in ents if (ent.label_ == 'GPE') ]) / len(words)\n",
    "        cols['named_entity_non_gpe_loc_pct'] = len([ ent for ent in ents if (ent.label_ == 'LOC') ]) / len(words)\n",
    "        cols['named_entity_product_pct'] = len([ ent for ent in ents if (ent.label_ == 'PRODUCT') ]) / len(words)\n",
    "        cols['named_entity_event_pct'] = len([ ent for ent in ents if (ent.label_ == 'EVENT') ]) / len(words)\n",
    "        cols['named_entity_woa_pct'] = len([ ent for ent in ents if (ent.label_ == 'WORK_OF_ART') ]) / len(words)\n",
    "        cols['named_entity_lang_pct'] = len([ ent for ent in ents if (ent.label_ == 'LANGUAGE') ]) / len(words)\n",
    "        cols['named_entity_date_pct'] = len([ ent for ent in ents if (ent.label_ == 'DATE') ]) / len(words)\n",
    "        cols['named_entity_time_pct'] = len([ ent for ent in ents if (ent.label_ == 'TIME') ]) / len(words)\n",
    "        cols['named_entity_money_pct'] = len([ ent for ent in ents if (ent.label_ == 'MONEY') ]) / len(words)\n",
    "        cols['named_entity_quantity_pct'] = len([ ent for ent in ents if (ent.label_ == 'QUANTITY') ]) / len(words)\n",
    "\n",
    "        rows.append(cols)\n",
    "        \n",
    "    return pd.DataFrame(rows, columns=cols.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, add_cols(train_df)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# train_df[train_df.named_entity_person_pct > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# train_df['cleaned_text'] = train_df.text.apply(\n",
    "#     lambda txt: ' '.join([ word.text for word in nlp(txt) if (not word.is_stop) ]))\n",
    "\n",
    "# train_df['lemmatized_text'] = train_df.text.apply(\n",
    "#     lambda txt: ' '.join([ word.lemma_ for word in nlp(txt) ]))\n",
    "\n",
    "# train_df['cleaned_lemmatized_text'] = train_df.cleaned_text.apply(\n",
    "#     lambda txt: ' '.join([ word.lemma_ for word in nlp(txt) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # counts\n",
    "# def char_count(row):\n",
    "#     return len(row['text'])\n",
    "\n",
    "# def word_count(row):\n",
    "#     tokens = [ token.text for token in nlp(row['text']) if (not token.is_punct) ]\n",
    "#     return len(tokens)\n",
    "\n",
    "# def u_word_pct(row):\n",
    "#     tokens = [ token.lemma_ for token in nlp(row['text']) if (not token.is_punct) ]\n",
    "#     return len(list(set(tokens))) / row['word_count']\n",
    "\n",
    "# # ratio of token types\n",
    "# def stopwords_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.is_stop) ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def punctuation_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.is_punct) ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def noun_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'NOUN') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def verb_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'VERB') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def adj_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'ADJ') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def proper_name_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'PROPN') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def symbol_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'SYM') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def number_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.pos_ == 'NUM') ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# def alpha_pct(row):\n",
    "#     tokens = [ token for token in nlp(row['text']) if (token.is_alpha) ]\n",
    "#     return len(tokens) / row['word_count']\n",
    "\n",
    "# # ratio of named entity types and specific named entities\n",
    "# def named_entity_pct(row):\n",
    "#     return len(nlp(row['text']).ents) / row['word_count']\n",
    "\n",
    "# def named_entity_person_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'PERSON')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_norp_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'NORP')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_facility_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'FACILITY')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_org_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'ORG')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_gpe_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'GPE')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_non_gpe_loc_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'LOC')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_product_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'PRODUCT')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_event_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'EVENT')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_woa_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'WORK_OF_ART')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_lang_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'LANGUAGE')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_date_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'DATE')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_time_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'TIME')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_money_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'MONEY')]\n",
    "#     return len(ents) / row['word_count']\n",
    "\n",
    "# def named_entity_quantity_pct(row):\n",
    "#     ents = [ ent for ent in nlp(row['text']).ents if (ent.label_ == 'QUANTITY')]\n",
    "#     return len(ents) / row['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # char and word counts\n",
    "# train_df['char_count'] = train_df.apply(lambda r: char_count(r), axis=1)\n",
    "# train_df['word_count'] = train_df.apply(lambda r: word_count(r), axis=1)\n",
    "\n",
    "# # unqique, stop, and punctuation percentages\n",
    "# train_df['u_word_pct'] = train_df.apply(lambda r: u_word_pct(r), axis=1)\n",
    "# train_df['stopwords_pct'] = train_df.apply(lambda r: stopwords_pct(r), axis=1)\n",
    "# train_df['punctuation_pct'] = train_df.apply(lambda r: punctuation_pct(r), axis=1)\n",
    "\n",
    "# # noun, verb, adj, proper name percentages\n",
    "# train_df['noun_pct'] = train_df.apply(lambda r: noun_pct(r), axis=1)\n",
    "# train_df['verb_pct'] = train_df.apply(lambda r: verb_pct(r), axis=1)\n",
    "# train_df['adj_pct'] = train_df.apply(lambda r: adj_pct(r), axis=1)\n",
    "# train_df['proper_name_pct'] = train_df.apply(lambda r: proper_name_pct(r), axis=1)\n",
    "\n",
    "# # alpha, number, symbol percentages\n",
    "# train_df['alpha_pct'] = train_df.apply(lambda r: alpha_pct(r), axis=1)\n",
    "# train_df['number_pct'] = train_df.apply(lambda r: number_pct(r), axis=1)\n",
    "# train_df['symbol_pct'] = train_df.apply(lambda r: symbol_pct(r), axis=1)\n",
    "\n",
    "# # named entity percentages\n",
    "# train_df['named_entity_pct'] = train_df.apply(lambda r: named_entity_pct(r), axis=1)\n",
    "# train_df['named_entity_person_pct'] = train_df.apply(lambda r: named_entity_person_pct(r), axis=1)\n",
    "# train_df['named_entity_norp_pct'] = train_df.apply(lambda r: named_entity_norp_pct(r), axis=1)\n",
    "# train_df['named_entity_facility_pct'] = train_df.apply(lambda r: named_entity_facility_pct(r), axis=1)\n",
    "# train_df['named_entity_org_pct'] = train_df.apply(lambda r: named_entity_org_pct(r), axis=1)\n",
    "# train_df['named_entity_gpe_pct'] = train_df.apply(lambda r: named_entity_gpe_pct(r), axis=1)\n",
    "# train_df['named_entity_non_gpe_loc_pct'] = train_df.apply(lambda r: named_entity_non_gpe_loc_pct(r), axis=1)\n",
    "# train_df['named_entity_product_pct'] = train_df.apply(lambda r: named_entity_product_pct(r), axis=1)\n",
    "# train_df['named_entity_event_pct'] = train_df.apply(lambda r: named_entity_event_pct(r), axis=1)\n",
    "# train_df['named_entity_woa_pct'] = train_df.apply(lambda r: named_entity_woa_pct(r), axis=1)\n",
    "# train_df['named_entity_lang_pct'] = train_df.apply(lambda r: named_entity_lang_pct(r), axis=1)\n",
    "# train_df['named_entity_date_pct'] = train_df.apply(lambda r: named_entity_date_pct(r), axis=1)\n",
    "# train_df['named_entity_time_pct'] = train_df.apply(lambda r: named_entity_time_pct(r), axis=1)\n",
    "# train_df['named_entity_money_pct'] = train_df.apply(lambda r: named_entity_money_pct(r), axis=1)\n",
    "# train_df['named_entity_quantity_pct'] = train_df.apply(lambda r: named_entity_quantity_pct(r), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define our multi-class logloss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# encode lables\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train_df.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define train and validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df.cleaned_lemmatized_text.values, y, \n",
    "                                                      stratify=y, test_size=0.1, \n",
    "                                                      random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**CountVectorizer** class to count how many times each term shows up in each document\n",
    "\n",
    "Parameters:\n",
    "- **min_df** (min. document frequency): The minimum number of document a term must be present in (integer or if float, represents the percentage of documents)\n",
    "\n",
    "- **max_df** (max. document frequency): The maximum number of documents a term can be found in (int or float, see above)\n",
    "\n",
    "Words that are too infrequent or too frequent lack predictive power.\n",
    "\n",
    "See: http://www.ultravioletanalytics.com/2016/11/18/tf-idf-basics-with-pandas-scikit-learn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), stop_words=STOP_WORDS)\n",
    "# ctv\n",
    "\n",
    "ctv.fit(list(X_train) + list(X_valid))\n",
    "X_train_ctv = ctv.transform(X_train)\n",
    "X_valid_ctv = ctv.transform(X_valid)\n",
    "\n",
    "len(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "list(islice(ctv.vocabulary_.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Transform the document into a **“bag of words”** representation which essentially is just a separate column for each term containing the count within each document. \n",
    "\n",
    "The **sparsity** of this representation which lets us know how many nonzero values there are in the dataset. The more sparse the data is the more challenging it will be to model, but that’s a discussion for another day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ctv_counts = ctv.transform(train_df.cleaned_lemmatized_text)\n",
    "print ('sparse matrix shape:', ctv_counts.shape)\n",
    "print ('nonzero count:', ctv_counts.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * ctv_counts.nnz / (ctv_counts.shape[0] * ctv_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# top 10 most common words\n",
    "occ = np.asarray(ctv_counts.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': ctv.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we’ve got term counts for each document we can use the TfidfTransformer to calculate the weights for each term in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(ctv_counts)\n",
    "transformed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# top 10 terms by average tf-idf weight\n",
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': ctv.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fit Logistic Regression on word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train_ctv, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_ctv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fit Naive Bayes on word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_ctv, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_ctv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**TF-IDF (Term Frequency - Inverse Document Frequency)**: A technique for determining what each document, in a set of documents, is about.\n",
    "\n",
    "**TF** (term frequency) = The porportion of occurences of a specific term to the total number of terms in a document\n",
    "\n",
    "**IDF** (inverse document frequency) = The inverse of the porportion of documents that contain a specifc word/phrase\n",
    "\n",
    "*The general idea is that if a specific phrase appears a lot of times in a given document, but it doesn’t appear in many other documents, then we have a good idea that the phrase is important in distinguishing that document from all the others.*\n",
    "\n",
    "For each term, we will have a separate feature (e.g., if there are 10k terms we will have 10k features), the value will be the tf-idf weight of that term in the document.\n",
    "\n",
    "Note: You want to use stopwords, stemming/lemmatization *first* to narrow down your corpus to the *important* words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=None, \n",
    "            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1, sublinear_tf=1,\n",
    "            stop_words=STOP_WORDS)\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(X_train) + list(X_valid))\n",
    "X_train_tfv =  tfv.transform(X_train) \n",
    "X_valid_tfv = tfv.transform(X_valid)\n",
    "\n",
    "X_train_tfv.shape, X_valid_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# top 5 weights for training set\n",
    "weights = np.asarray(X_train_tfv.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': tfv.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fit simple logistic regression on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train_tfv, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_tfv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fit Naive Bayes on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfv, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_tfv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SVM\n",
    "\n",
    "Since SVMs take a lot of time, we will reduce the number of features from the TF-IDF using Singular Value Decomposition before applying SVM.\n",
    "\n",
    "Also, note that before applying SVMs, we must standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# apply SVD, I chose 120 components (120-200 components are good enough for SVM model)\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(X_train_tfv)\n",
    "X_train_svd = svd.transform(X_train_tfv)\n",
    "X_valid_svd = svd.transform(X_valid_tfv)\n",
    "\n",
    "# scale the data obtained from SVD ... renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(X_train_svd)\n",
    "X_train_svd_scl = scl.transform(X_train_svd)\n",
    "X_valid_svd_scl = scl.transform(X_valid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(X_train_svd_scl, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on word counts\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(X_train_ctv.tocsc(), y_train)\n",
    "preds = clf.predict_proba(X_valid_ctv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(X_train_tfv.tocsc(), y_train)\n",
    "preds = clf.predict_proba(X_valid_tfv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on SVD features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(X_train_svd, y_train)\n",
    "preds = clf.predict_proba(X_valid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a VERY simple xgboost on SVD features\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "\n",
    "clf.fit(X_train_svd, y_train)\n",
    "preds = clf.predict_proba(X_valid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try using more of the engineered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'author', 'text', 'cleaned_text', 'lemmatized_text', 'cleaned_lemmatized_text']\n",
    "train = train_df.drop(columns=cols_to_drop).as_matrix()\n",
    "\n",
    "# fit CountVectorizer on ENTIRE training dataset\n",
    "ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), stop_words=STOP_WORDS, \n",
    "                      min_df=0.0001)\n",
    "ctv.fit(train_df.cleaned_lemmatized_text.values)\n",
    "train_ctv = ctv.transform(train_df.cleaned_lemmatized_text.values)\n",
    "print(train_ctv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! NOTE: train_ctv will be a sparse array, so to concatenate you have to cast it to \".toarray()\n",
    "train = np.concatenate([train, train_ctv.toarray()], axis=1)\n",
    "print(train.shape)\n",
    "\n",
    "# define train and validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train, y, \n",
    "                                                      stratify=y, test_size=0.1, \n",
    "                                                      random_state=42, shuffle=True)\n",
    "# fit model\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'author', 'text', 'cleaned_text', 'lemmatized_text', 'cleaned_lemmatized_text']\n",
    "train = train_df.drop(columns=cols_to_drop).as_matrix()\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=None, \n",
    "            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1, sublinear_tf=1,\n",
    "            stop_words=STOP_WORDS)\n",
    "\n",
    "tfv.fit(train_df.cleaned_lemmatized_text.values)\n",
    "train_tfv = ctv.transform(train_df.cleaned_lemmatized_text.values)\n",
    "print(train_tfv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! NOTE: train_ctv will be a sparse array, so to concatenate you have to cast it to \".toarray()\n",
    "train = np.concatenate([train, train_tfv.toarray()], axis=1)\n",
    "print(train.shape)\n",
    "\n",
    "# define train and validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train, y, \n",
    "                                                      stratify=y, test_size=0.1, \n",
    "                                                      random_state=42, shuffle=True)\n",
    "# fit model\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict_proba(X_valid)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_valid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
