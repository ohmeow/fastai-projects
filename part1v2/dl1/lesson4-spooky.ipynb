{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle\n",
    "import itertools as it\n",
    "from operator import itemgetter\n",
    "\n",
    "from spooky import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/spooky'\n",
    "\n",
    "os.makedirs(f'{PATH}/models', exist_ok=True)\n",
    "os.makedirs(f'{PATH}/tmp', exist_ok=True)\n",
    "\n",
    "bs = 4\n",
    "bptt = 5\n",
    "\n",
    "# for NLP, configure Adam to use less momentum than the defaul of 0.9\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 8392)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get raw training and test datasets\n",
    "train_raw_df = pd.read_csv(f'{PATH}/train.csv')\n",
    "test_df = pd.read_csv(f'{PATH}/test.csv')\n",
    "\n",
    "len(train_raw_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a baseline\n",
    "\n",
    "Running with default parameters to figure out good default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard CV\n",
    "\n",
    "To build a standard cross-validation dataset use this\n",
    "val_idxs = get_cv_idxs(len(train_raw_df), val_pct=0.10)\n",
    "\n",
    "train_df =  train_raw_df.drop(val_idxs)\n",
    "val_df = train_raw_df.iloc[val_idxs]\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize = split each sentence into a list of words\n",
    "' '.join(spacy_tok(train_df.text.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#createa torchtext field = describes how to preprocess a piece of text\n",
    "txt_fld = data.Field(lower=True, tokenize=spacy_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = dict(train_df=train_df, val_df=val_df, test_df=test_df)\n",
    "\n",
    "# min_freq = 10 says, \"treat any word that appears less than 10 times as the word <unk>\"\n",
    "md = LanguageModelData.from_dataframes(PATH, txt_fld, 'text', **dataframes, \n",
    "                                       bs=bs, bptt=bptt, min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after building the ModelData object, TEXT.vocab is set.  because this will be needed again, save it\n",
    "pickle.dump(txt_fld, open(f'{PATH}/models/TEXT.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batches, # of unique tokens in vocab, # of items in ds, # of words in ds\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int to string mapping\n",
    "TEXT.vocab.itos[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string to int mapping\n",
    "TEXT.vocab.stoi['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a LanguageModelData object there is only one item in each dataset: all the words joined together\n",
    "md.trn_ds[0].text[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext will handle turning this words into integer Ids\n",
    "TEXT.numericalize([md.trn_ds[0].text[:12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(md.trn_dl))\n",
    "print(batch[0].size()), print(batch[1].size())\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 200  # size of each embedding vector\n",
    "n_hidden = 500     # number of hidden activations per layer\n",
    "n_layers = 3       # number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, emb_sz, n_hidden, n_layers,\n",
    "                      dropouti=0.1, dropout=0.1, wdrop=0.2, dropoute=0.04, dropouth=0.1)\n",
    "\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf = learner.lr_find() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lr, 4, wds=wds, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('spooky_adam_enc1')\n",
    "# learner.load_encoder('spooky_adam1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lr, 2, wds=wds, cycle_len=5, cycle_save_name='spooky_adam_enc2_c1_cl5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('spooky_adam_enc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lr, 1, wds=wds, cycle_len=10, cycle_save_name='spooky_adam_enc3_c1_cl10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('spooky_adam_enc3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric perplexity (how language model accuracy generally measured) = exp() of loss function\n",
    "np.exp(4.33935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "\n",
    "Do a grid search to figure out params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV ...\n",
    "\n",
    "# 1. Define hyper parameters\n",
    "# size of each embedding vector, # of hidden activations per layer, # of layers, min word freq\n",
    "params = { 'emb_sz': [50, 200, 400], 'n_hidden': [512, 1024], 'n_layers': [3, 4], 'min_freq': [10] }\n",
    "\n",
    "# 2. Define folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# get folds\n",
    "kfolds = [ (train_idxs, val_idxs) for train_idxs, val_idxs in skf.split(train_raw_df.id, train_raw_df.author) ]\n",
    "\n",
    "# 3. Get all permutations of hyperparameters\n",
    "param_names = sorted(params)\n",
    "param_combos = [dict(zip(param_names, prod)) for prod in it.product(*(params[k] for k in param_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "wds = 1e-6\n",
    "\n",
    "for hps in param_combos:\n",
    "    # get params for this run\n",
    "    emb_sz, n_hidden, n_layers, min_freq = itemgetter('emb_sz', 'n_hidden', 'n_layers', 'min_freq')(hps)\n",
    "\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for f in kfolds:\n",
    "        # build train/val dataframes\n",
    "        train_df =  train_raw_df.iloc[f[0]]\n",
    "        val_df = train_raw_df.iloc[f[1]]\n",
    "        \n",
    "        #create torchtext field = describes how to preprocess a piece of text\n",
    "        txt_fld = data.Field(lower=True, tokenize=spacy_tok)\n",
    "        \n",
    "        dataframes = dict(train_df=train_df, val_df=val_df, test_df=test_df)\n",
    "\n",
    "        # min_freq = 10 says, \"treat any word that appears less than 10 times as the word <unk>\"\n",
    "        md = LanguageModelData.from_dataframes(PATH, txt_fld, 'text', **dataframes, \n",
    "                                               bs=bs, bptt=bptt, min_freq=min_freq)\n",
    "        \n",
    "        learner = md.get_model(opt_fn, emb_sz, n_hidden, n_layers,\n",
    "                      dropouti=0.1, dropout=0.1, wdrop=0.2, dropoute=0.04, dropouth=0.1)\n",
    "\n",
    "        learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "        learner.clip = 0.3\n",
    "        \n",
    "        learner.fit(lr, 4, wds=wds, cycle_len=1, cycle_mult=2)\n",
    "        \n",
    "        learner.fit(lr, 2, wds=wds, cycle_len=5)\n",
    "        \n",
    "        learner.fit(lr, 1, wds=wds, cycle_len=10)\n",
    "        \n",
    "        acc = accuracy(*learner.predict_with_targs())\n",
    "        fold_metrics.append(acc)\n",
    "        \n",
    "    hps['metrics'] = np.mean(fold_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final LanguageModel\n",
    "\n",
    "Use the best hyperparameters against the full training dataset to train a final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on entire training dataset with best metrics\n",
    "train_df = train_raw_df.copy()\n",
    "val_df = train_raw_df.copy()\n",
    "\n",
    "lr = 3e-3\n",
    "wds = 1e-6\n",
    "\n",
    "best_params = sorted(params, key=lambda k: k['metrics'])[0]\n",
    "emb_sz, n_hidden, n_layers, min_freq = itemgetter('emb_sz', 'n_hidden', 'n_layers', 'min_freq')(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a short bit of text to \"prime\" the precitions, then use torchtext to numericalize it\n",
    "# so we can feed it into our language model\n",
    "m = learner.model\n",
    "ss = \"\"\". It was a dark and scary night. The old\"\"\"\n",
    "s = [spacy_tok(ss)]\n",
    "t = TEXT.numericalize(s)\n",
    "' '.join(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[0].bs = 1      # set batch size = 1\n",
    "m.eval()         # turn-off dropout\n",
    "m.reset()        # reset hidden state\n",
    "res, *_ = m(t)   # get predictions from model\n",
    "m[0].bs = bs     # put batch size back to what it was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 predictions for next word\n",
    "nexts = torch.topk(res[-1], 10)[1]\n",
    "[TEXT.vocab.itos[o] for o in to_np(nexts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to generate more text\n",
    "print(ss, \"\\n\")\n",
    "\n",
    "for i in range(50):\n",
    "    n = res[-1].topk(2)[1]\n",
    "    n = n[1] if n.data[0] == 0 else n[0]\n",
    "    print(TEXT.vocab.itos[n.data[0]], end=' ')\n",
    "    res, *_ = m(n[0].unsqueeze(0))\n",
    "    \n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "bptt = 70\n",
    "\n",
    "emb_sz = 400       # size of each embedding vector\n",
    "nh = 1024           # of hidden activations per layer\n",
    "nl = 3             # of layers\n",
    "\n",
    "# for NLP, configure Adam to use less momentum than the defaul of 0.9\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same vocab built from the language model so as to ensure words map to same Ids\n",
    "TEXT = pickle.load(open(f'{PATH}/models/TEXT.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR_LABEL = data.Field(sequential=False)\n",
    "splits = SpookyDataset.splits(TEXT, AUTHOR_LABEL, train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = splits[0].examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.label, ' '.join(t.text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai can create a ModelData object directly from torchtext splits\n",
    "md2 = TextData.from_splits(PATH, splits, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=emb_sz, n_hid=nh, n_layers=nl,\n",
    "                      dropout=0.1, dropouti=0.4, wdrop=0.5, dropoute=0.05, dropouth=0.3)\n",
    "\n",
    "m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "m3.load_encoder(f'spooky_adam_enc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.clip = 25.\n",
    "lrs = np.array([1e-4, 1e-3, 1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.freeze_to(-1) # freeze everything except last layer\n",
    "m3.fit(lrs/2, 2, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.unfreeze()\n",
    "m3.fit(lrs, 2, metrics=[accuracy], cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.fit(lrs/2, 4, metrics=[accuracy], cycle_len=1, cycle_mult=2, cycle_save_name='spooky_sent1_c4_cl1x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.fit(lrs/4, 3, metrics=[accuracy], cycle_len=3, cycle_save_name='spooky_sent2_c3_cl3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.load_cycle('spooky_sent1_c4_cl1x2', 1) # NOTE: using model with lower val loss is better\n",
    "# m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = AUTHOR_LABEL.vocab.itos\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "m = m3.model \n",
    "m[0].bs = 1\n",
    "for index, row in test_df.iterrows():\n",
    "    ss = row['text']\n",
    "    s = [spacy_tok(ss)]\n",
    "    t = TEXT.numericalize(s)\n",
    "   \n",
    "    m.eval()\n",
    "    m.reset()\n",
    "    res,*_ = m(t)\n",
    "    preds.append(to_np(res).squeeze()[1:])\n",
    "#     preds.append(to_np(res).squeeze())\n",
    "    \n",
    "preds = np.array(preds)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = to_np(F.softmax(torch.from_numpy(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_clip(arr, mx):\n",
    "    clipped = np.clip(arr, (1-mx)/1, mx)\n",
    "    return clipped/clipped.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = do_clip(probs, 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_df = test_df.copy()\n",
    "preds_test_df['EAP'] = probs[:,0]\n",
    "preds_test_df['MWS'] = probs[:,1]\n",
    "preds_test_df['HPL'] = probs[:,2]\n",
    "\n",
    "preds_test_df.drop('text', axis=1, inplace=True)\n",
    "preds_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_df.to_csv(f'{PATH}/subm_wg_20171127_4.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_df = pd.read_csv(f'{PATH}/subm_wg_20171126_3.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(f'{PATH}/subm_wg_20171126_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
