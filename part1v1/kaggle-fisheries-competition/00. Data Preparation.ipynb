{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Download bounding box data from here: https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/discussion/25902\n",
    "\n",
    "Notes: \n",
    "\n",
    "After running this notebook you can verify counts using > *`ls | wc -l`* (example: `ls train/* | wc -l` will give you a count of all the images in the training subdirectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../util'))\n",
    "\n",
    "# core imports\n",
    "from keras_tf_util import *\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure various jupyter defaults\n",
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# configure autoreload to automatically reload modules when files are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "DATA_HOME_DIR = current_dir + '/data/'\n",
    "\n",
    "n_validation_files = 500\n",
    "n_sample_train_files = 400\n",
    "n_sample_val_files = 200\n",
    "\n",
    "rebuild_from_data_download = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (rebuild_from_data_download == True):\n",
    "    # cleanup\n",
    "    if (os.path.exists(DATA_HOME_DIR + 'train')): shutil.rmtree(DATA_HOME_DIR + 'train')\n",
    "    if (os.path.exists(DATA_HOME_DIR + 'test')): shutil.rmtree(DATA_HOME_DIR + 'test')\n",
    "    if (os.path.exists(DATA_HOME_DIR + 'val')): shutil.rmtree(DATA_HOME_DIR + 'val')\n",
    "    if (os.path.exists(DATA_HOME_DIR + 'sample')): shutil.rmtree(DATA_HOME_DIR + 'sample')\n",
    "    \n",
    "    # unzip training and test datasets\n",
    "    with zipfile.ZipFile(DATA_HOME_DIR + 'train.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_HOME_DIR)\n",
    "        \n",
    "    with zipfile.ZipFile(DATA_HOME_DIR + 'test_stg1.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_HOME_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation, test, and sample directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (rebuild_from_data_download == True):\n",
    "    g = glob(DATA_HOME_DIR + 'train/*')\n",
    "    \n",
    "    # validation directories\n",
    "    for d in g: make_dir(DATA_HOME_DIR + 'valid/' + os.path.basename(d))\n",
    "\n",
    "    # test\n",
    "    make_dir(DATA_HOME_DIR + 'test_stg1/unknown')\n",
    "\n",
    "    # sample\n",
    "    for d in g:\n",
    "        make_dir(DATA_HOME_DIR + 'sample/train/' + os.path.basename(d))\n",
    "        make_dir(DATA_HOME_DIR + 'sample/valid/' + os.path.basename(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move validation and test data into appropriate sub-directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (rebuild_from_data_download == True):\n",
    "    # move n_validation_files from TRAINING into VALIDATION \n",
    "    g = glob(DATA_HOME_DIR + 'train/*/*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    \n",
    "    for i in range(n_validation_files): \n",
    "        new_path = '{0}/{1}'.format(os.path.basename(os.path.dirname(shuf[i])), os.path.basename(shuf[i]))\n",
    "        os.rename(shuf[i], DATA_HOME_DIR + 'valid/' + new_path)\n",
    "        \n",
    "    # move TEST images into /unknown subdirectory\n",
    "    g = glob(DATA_HOME_DIR + 'test_stg1/*')\n",
    "    for f in g: shutil.move(f, DATA_HOME_DIR + 'test_stg1/unknown')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy subset of training and validation data into /sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (rebuild_from_data_download == True):\n",
    "    # copy n_sample_train_files from TRAINING into SAMPLE/TRAIN\n",
    "    g = glob(DATA_HOME_DIR + 'train/*/*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    \n",
    "    for i in range(n_sample_train_files): \n",
    "        new_path = '{0}/{1}'.format(os.path.basename(os.path.dirname(shuf[i])), os.path.basename(shuf[i]))\n",
    "        shutil.copyfile(shuf[i], DATA_HOME_DIR + 'sample/train/' + new_path)\n",
    "        \n",
    "    # copy n_sample_val_files from VALIDATION into SAMPLE/VALID \n",
    "    g = glob(DATA_HOME_DIR + 'valid/*/*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    \n",
    "    for i in range(n_sample_val_files):\n",
    "        new_path = '{0}/{1}'.format(os.path.basename(os.path.dirname(shuf[i])), os.path.basename(shuf[i]))\n",
    "        shutil.copyfile(shuf[i], DATA_HOME_DIR + 'sample/valid/' + new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
