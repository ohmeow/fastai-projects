{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "import dill as pickle\n",
    "\n",
    "import pdb\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "# pandas and plotting config\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_labels_from_folders(path, classes):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(classes):\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            texts.append(fname.open('r').read())\n",
    "            labels.append(idx)\n",
    "            \n",
    "    return np.array(texts),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(row, txt_cols, lbl_cols=[], lbl_dtype=np.int64):\n",
    "    n_txt_cols = len(txt_cols)\n",
    "    n_label_cols = len(lbl_cols)\n",
    "    \n",
    "    labels = row[lbl_cols].values.astype(lbl_dtype) if (n_label_cols > 0) else []\n",
    "    \n",
    "    docs = f'\\n{BOS} {FLD} 1 ' + row[txt_cols[0]].astype(str)\n",
    "    for i, col in enumerate(txt_cols[1:]):\n",
    "        docs += f' {FLD} {i+2} ' + row[col].astype(str)\n",
    "\n",
    "    docs = docs.apply(clean_text).values.astype(str)\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(docs))\n",
    "    \n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_examples(df, txt_cols, lbl_cols=[], lbl_dtype=np.int64):\n",
    "    tok, labels = [], []\n",
    "    \n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = process_example(r, txt_cols, lbl_cols, lbl_dtype)\n",
    "        tok += tok_\n",
    "        labels += labels_\n",
    "        \n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens, min_freq=1, max_size=None, \n",
    "                 specials=['_unk_', '_pad_', '_bos_', '_eos_'], unk_idx=0):\n",
    "        \n",
    "        self.min_freq = max(min_freq, 1)\n",
    "        self.specials = specials\n",
    "        self.unk_idx = unk_idx\n",
    "        \n",
    "        self.tokens = list(specials)\n",
    "        self.max_size = None if max_size is None else max_size + len(self.tokens)\n",
    "        \n",
    "        self.token_freqs = Counter(tokens)\n",
    "        for t in self.specials: del self.token_freqs[t]\n",
    "            \n",
    "        self.tokens = [ t for t, c in self.token_freqs.most_common(self.max_size) if c > min_freq ]\n",
    "        \n",
    "        #itos\n",
    "        self.itos = self.tokens\n",
    "        \n",
    "        #stoi\n",
    "        self.stoi = collections.defaultdict(lambda: self.unk_idx, { tok:i for i, tok in enumerate(self.tokens) })\n",
    "        \n",
    "    def get_unk_idx(self):\n",
    "        return self.unk_idx\n",
    "    \n",
    "    def token_freq(self, token):\n",
    "        return self.token_freqs.get(token, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, docs, vocab=None, min_freq=1, max_size=None):\n",
    "        self.tokens = []\n",
    "        for d in docs: self.tokens += d\n",
    "        \n",
    "        if (vocab):\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = Vocab(self.tokens, min_freq, max_size)\n",
    "        \n",
    "        self.data = np.array([[ self.vocab.stoi[t] for t in self.tokens ]])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## IMDB - Multi-classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH= Path('data/aclImdb')\n",
    "TRN_PATH = PATH/'train'\n",
    "VAL_PATH = PATH/'test'\n",
    "\n",
    "LM_PATH = PATH/'imdb_lm'\n",
    "CLS_PATH = PATH/'imdb_class'\n",
    "\n",
    "(LM_PATH/'models').mkdir(parents=True, exist_ok=True)\n",
    "(LM_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "(CLS_PATH/'models').mkdir(parents=True, exist_ok=True)\n",
    "(CLS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "# [child for child in PATH.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%ls {str(PATH)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get list of documents and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CLASSES = ['neg', 'pos', 'unsup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_docs, trn_labels = texts_labels_from_folders(TRN_PATH, CLASSES)\n",
    "val_docs, val_labels = texts_labels_from_folders(VAL_PATH, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(trn_docs), len(val_docs), len(trn_labels[trn_labels == 1]), len(trn_labels[trn_labels == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Randomize ordering of everythig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "trn_idxs = np.random.permutation(len(trn_docs))\n",
    "val_idxs = np.random.permutation(len(val_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_docs = trn_docs[trn_idxs]\n",
    "trn_labels = trn_labels[trn_idxs]\n",
    "\n",
    "val_docs = val_docs[val_idxs]\n",
    "val_labels = val_labels[val_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Put documents and labels into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "col_names = ['labels', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_df = pd.DataFrame({'text':trn_docs, 'labels':trn_labels}, columns=col_names)\n",
    "val_df = pd.DataFrame({'text':val_docs, 'labels':val_labels}, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display(trn_df.head(1))\n",
    "display(trn_df.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_df[trn_df['labels'] != 2].to_csv(CLS_PATH/'train.csv', index=False)\n",
    "val_df.to_csv(CLS_PATH/'test.csv', index=False)\n",
    "\n",
    "(CLS_PATH/'classes.txt').open('w').writelines(f'{c}\\n' for c in CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a language model, we want to use the entire corpus.  In the case of IMDB, only 50k of the 100k documents are labeled and so we look to the `train/all` and `test/all` folders to graby all 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_docs, val_docs = sklearn.model_selection.train_test_split(\n",
    "    np.concatenate([trn_docs, val_docs]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(trn_docs), len(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_df = pd.DataFrame({'text':trn_docs}, columns=col_names[1:])\n",
    "val_df = pd.DataFrame({'text':val_docs}, columns=col_names[1:])\n",
    "\n",
    "trn_df.to_csv(LM_PATH/'train.csv', index=False)\n",
    "val_df.to_csv(LM_PATH/'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Clean and tokenize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chunksize=24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_df = pd.read_csv(LM_PATH/'train.csv', chunksize=chunksize)\n",
    "val_df = pd.read_csv(LM_PATH/'test.csv', chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_toks, _ = process_examples(trn_df, col_names[1:])\n",
    "val_toks, _ = process_examples(val_df, col_names[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(trn_toks), len(val_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(trn_toks[0][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'trn_toks.npy', trn_toks)\n",
    "np.save(LM_PATH/'tmp'/'val_toks.npy', val_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_toks = np.load(LM_PATH/'tmp'/'trn_toks.npy')\n",
    "val_toks = np.load(LM_PATH/'tmp'/'val_toks.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define the vocab and build the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time trn_ds = LanguageDataset(np.concatenate((trn_toks, val_toks)), min_freq=min_freq, max_size=max_vocab)\n",
    "%time val_ds = LanguageDataset(val_toks, vocab=trn_ds.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(trn_ds[0]), len(trn_ds.tokens), len(trn_ds.vocab.tokens), len(trn_ds))\n",
    "print(len(val_ds[0]), len(val_ds.tokens), len(val_ds.vocab.tokens), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(trn_ds[0][:10])\n",
    "print([ trn_ds.vocab.itos[idx] for idx in trn_ds[0][:10] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time pickle.dump(trn_ds, open(LM_PATH/'tmp'/'trn_ds.pkl', 'wb'))\n",
    "%time pickle.dump(val_ds, open(LM_PATH/'tmp'/'val_ds.pkl', 'wb'))\n",
    "%time pickle.dump(trn_ds.vocab, open(LM_PATH/'tmp'/'vocab.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trn_ds = pickle.load(open(LM_PATH/'tmp'/'trn_ds.pkl', 'rb'))\n",
    "val_ds = pickle.load(open(LM_PATH/'tmp'/'val_ds.pkl', 'rb'))\n",
    "vocab = pickle.load(open(LM_PATH/'tmp'/'vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Configure and build the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bptt = 70\n",
    "bsz = 52\n",
    "wd = 1e-7\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_dl = LanguageModelLoader(trn_ds[0], bsz, bptt)\n",
    "val_dl = LanguageModelLoader(val_ds[0], bsz, bptt)\n",
    "\n",
    "md = LanguageModelData(PATH, 1, len(vocab.tokens), trn_dl, val_dl, bs=bsz, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(md.trn_dl), md.n_tok, len(trn_ds), len(trn_ds.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(md.trn_dl))\n",
    "print(batch[0].size()), print(batch[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_df = pd.read_csv(CLS_PATH/'train.csv', chunksize=chunksize)\n",
    "val_df = pd.read_csv(CLS_PATH/'test.csv', chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_toks, trn_labels = process_examples(trn_df, col_names[1:], col_names[:1])\n",
    "val_toks, val_labels = process_examples(val_df, col_names[1:], col_names[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(CLS_PATH/'tmp'/'trn_toks.npy', trn_toks)\n",
    "np.save(CLS_PATH/'tmp'/'val_toks.npy', val_toks)\n",
    "\n",
    "np.save(CLS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
    "np.save(CLS_PATH/'tmp'/'val_labels.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_toks = np.load(CLS_PATH/'tmp'/'trn_toks.npy')\n",
    "val_toks = np.load(CLS_PATH/'tmp'/'val_toks.npy')\n",
    "\n",
    "trn_labels = np.load(CLS_PATH/'tmp'/'trn_labels.npy')\n",
    "val_labels = np.load(CLS_PATH/'tmp'/'val_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq = Counter(p for o in trn_toks for p in o)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time vocab = pickle.load(open(LM_PATH/'tmp'/'vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_nums = np.array([[vocab.stoi[o] for o in p] for p in trn_toks])\n",
    "val_nums = np.array([[vocab.stoi[o] for o in p] for p in val_toks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(trn_nums[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(CLS_PATH/'tmp'/'trn_nums.npy', trn_nums)\n",
    "np.save(CLS_PATH/'tmp'/'val_nums.npy', val_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_nums = np.load(CLS_PATH/'tmp'/'trn_nums.npy')\n",
    "val_nums = np.load(CLS_PATH/'tmp'/'val_nums.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_labels = np.squeeze(np.load(CLS_PATH/'tmp'/'trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(CLS_PATH/'tmp'/'val_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_labels.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Make sure labels are zero-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "min_lbl = trn_labels.min()\n",
    "trn_labels -= min_lbl\n",
    "val_labels -= min_lbl\n",
    "c=int(trn_labels.max())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bptt, em_sz, nh, nl = 70,400,1150,3\n",
    "bsz = 48\n",
    "vs = len(vocab.tokens)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_ds = TextDataset(trn_nums, trn_labels)\n",
    "val_ds = TextDataset(val_nums, val_labels)\n",
    "\n",
    "trn_samp = SortishSampler(trn_nums, key=lambda x: len(trn_nums[x]), bs=bsz//2)\n",
    "val_samp = SortSampler(val_nums, key=lambda x: len(val_nums[x]))\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, bsz//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bsz, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "\n",
    "md = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(trn_ds), trn_dl.batch_size, len(trn_dl), len(trn_dl.dataset), len(trn_ds[0][0]), trn_ds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(x.size(), x.type(), y.size(), y.type(), bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOXIC- Multi-label problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH= Path('data/toxic-comment')\n",
    "\n",
    "(PATH/'models').mkdir(parents=True, exist_ok=True)\n",
    "(PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "# [child for child in PATH.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df = pd.read_csv(PATH/'train.csv')\n",
    "test_df = pd.read_csv(PATH/'test.csv')\n",
    "sample_subm_df = pd.read_csv(PATH/'sample_submission.csv')\n",
    "\n",
    "txt_col = 'comment_text'\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "raw_train_df['none'] = 1 - raw_train_df[label_cols].max(axis=1)\n",
    "\n",
    "model_cols = ['id', txt_col] + label_cols + ['none']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91058 4793 9288 502\n"
     ]
    }
   ],
   "source": [
    "# split the training data into a train and validatin dataset\n",
    "trn, val = train_test_split(raw_train_df, test_size=0.05, random_state=9)\n",
    "print(len(trn), len(val), len(trn[trn.none != 1]), len(val[val.none != 1]))\n",
    "\n",
    "# save train, val, and test datasets for torchtext\n",
    "trn[model_cols].to_csv(PATH/'train_ds.csv', index=None)\n",
    "val[model_cols].to_csv(PATH/'valid_ds.csv', index=None)\n",
    "\n",
    "# save full cleaned datasets (train+valid and test) as well\n",
    "raw_train_df[model_cols].to_csv(PATH/'full_train_ds.csv', index=None)\n",
    "test_df[['id', txt_col]].to_csv(PATH/'test_ds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you did with this edit to W. S. Merwin. If you continue to do so, you will be blocked from editing.    \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "0  22256635   \n",
       "1  27450690   \n",
       "\n",
       "                                                                                                                                       comment_text  \\\n",
       "0  Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.                                                                \n",
       "1  \"\\n\\n Please do not vandalize pages, as you did with this edit to W. S. Merwin. If you continue to do so, you will be blocked from editing.    \"   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  none  \n",
       "0  1      0             0        0       0       0              0     \n",
       "1  0      0             0        0       0       0              1     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. SS500 .jpg)==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the area is often referred to as this, it (in my opinion) has never held the encyclopedic precision of \"Louisville metropolitian area\", which has a specific U.S. Census definition.  Also, apparently Kentuckiana often refers to the local television viewing area, which isn't nearly contiguous with the official metro area.  As you indicate, Kentuckiana seems to be more of a slang or marketing phenomena than anything we could pin down in encyclopedic terms here.  That's why we see Wikipedia language like \"the Louisville metropolitan area, sometimes referred to as Kentuckiana\". That's my take on it. —   •</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  6044863   \n",
       "1  6102620   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                comment_text  \n",
       "0  ==Orphaned non-free media (Image:41cD1jboEvL. SS500 .jpg)==                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1  ::Kentuckiana is colloquial.  Even though the area is often referred to as this, it (in my opinion) has never held the encyclopedic precision of \"Louisville metropolitian area\", which has a specific U.S. Census definition.  Also, apparently Kentuckiana often refers to the local television viewing area, which isn't nearly contiguous with the official metro area.  As you indicate, Kentuckiana seems to be more of a slang or marketing phenomena than anything we could pin down in encyclopedic terms here.  That's why we see Wikipedia language like \"the Louisville metropolitan area, sometimes referred to as Kentuckiana\". That's my take on it. —   •  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.read_csv(PATH/\"full_train_ds.csv\").head(2))\n",
    "display(pd.read_csv(PATH/\"test_ds.csv\").head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and tokenize documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000 #30000\n",
    "min_freq = 10 #0\n",
    "max_len = 175 #100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df = pd.read_csv(PATH/'train_ds.csv', chunksize=chunksize)\n",
    "val_df = pd.read_csv(PATH/'valid_ds.csv', chunksize=chunksize)\n",
    "test_df = pd.read_csv(PATH/'test_ds.csv', chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "trn_toks, trn_labels = process_examples(trn_df, [txt_col], label_cols, lbl_dtype=np.float32)\n",
    "val_toks, val_labels = process_examples(val_df, [txt_col], label_cols, lbl_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91058, 4793, (6,), (6,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_toks), len(val_toks), trn_labels[0].shape, val_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 297273),\n",
       " ('the', 283109),\n",
       " (',', 269182),\n",
       " ('\"', 214611),\n",
       " ('to', 169607),\n",
       " ('\\n', 140276),\n",
       " ('i', 137007),\n",
       " ('of', 128229),\n",
       " ('and', 127863),\n",
       " ('you', 125563)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = Counter(p for o in trn_toks for p in o)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocab, fix lengths of each document, and numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_toks = []\n",
    "for toks in np.concatenate((trn_toks, val_toks)): all_toks += toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(all_toks, min_freq, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_toks = [ d[:max_len] + ['_pad_']*(max_len-len(d)) for d in trn_toks ]\n",
    "val_toks = [ d[:max_len] + ['_pad_']*(max_len-len(d)) for d in val_toks ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(PATH/'tmp'/'trn_toks.npy', trn_toks)\n",
    "# np.save(PATH/'tmp'/'val_toks.npy', val_toks)\n",
    "\n",
    "np.save(PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
    "np.save(PATH/'tmp'/'val_labels.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_toks = np.load(PATH/'tmp'/'trn_toks.npy')\n",
    "# val_toks = np.load(PATH/'tmp'/'val_toks.npy')\n",
    "\n",
    "trn_labels = np.load(PATH/'tmp'/'trn_labels.npy')\n",
    "val_labels = np.load(PATH/'tmp'/'val_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_nums = np.array([ [vocab.stoi[o] for o in p] for p in trn_toks ])\n",
    "val_nums = np.array([ [vocab.stoi[o] for o in p] for p in val_toks ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATH/'tmp'/'trn_nums.npy', trn_nums)\n",
    "np.save(PATH/'tmp'/'val_nums.npy', val_nums)\n",
    "pickle.dump(vocab, open(PATH/'tmp'/'vocab.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91058, 4793, 175)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_toks), len(val_toks), len(trn_toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((91058, 6), (4793, 6))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.squeeze(trn_labels)).shape, (np.squeeze(val_labels)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_nums = np.load(PATH/'tmp'/'trn_nums.npy')\n",
    "val_nums = np.load(PATH/'tmp'/'val_nums.npy')\n",
    "vocab = pickle.load(open(PATH/'tmp'/'vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_labels = np.squeeze(np.load(PATH/'tmp'/'trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(PATH/'tmp'/'val_labels.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build datasets and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 64\n",
    "pretrained_vectors = None #'fasttext.en.300d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = TextDataset(trn_nums, trn_labels)\n",
    "val_ds = TextDataset(val_nums, val_labels)\n",
    "\n",
    "trn_samp = SortishSampler(trn_nums, key=lambda x: len(trn_nums[x]), bs=bsz//2)\n",
    "val_samp = SortSampler(val_nums, key=lambda x: len(val_nums[x]))\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, bsz//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bsz, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "\n",
    "md = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91058, 32, 2846, 91058, 175, array([0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_ds), trn_dl.batch_size, len(trn_dl), len(trn_dl.dataset), len(trn_ds[0][0]), trn_ds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175, 32]) torch.LongTensor torch.Size([32, 6]) torch.FloatTensor 64\n"
     ]
    }
   ],
   "source": [
    "print(x.size(), x.type(), y.size(), y.type(), bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this against a simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLstm(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_sz=300, n_rnn_hidden=256, n_rnn_layers=1, bi_dir=True, out_sz=1, bsz=64,\n",
    "                 dropout_rnn=0.3, dropout_after_emb=0.4, dropout_emb=0.1, wdrop=0.05):\n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        self.bsz = bsz\n",
    "               \n",
    "        # configure embeddings layer\n",
    "        self.dropout_emb = dropout_emb\n",
    "        self.dropout_after_emb = LockedDropout(dropout_after_emb)\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz)\n",
    "#         self.emb.data = train_ds.fields[txt_col].vocab.vectors # to use the pretrained vectors\n",
    "        self.emb_with_drop = EmbeddingDropout(self.emb)\n",
    "        \n",
    "        # configure rnns\n",
    "        self.n_rnn_hidden, self.n_rnn_layers, self.n_dirs = n_rnn_hidden, n_rnn_layers, 2 if bi_dir else 1\n",
    "        self.rnn = nn.LSTM(emb_sz, self.n_rnn_hidden, self.n_rnn_layers, bidirectional=bi_dir, dropout=dropout_rnn)\n",
    "        if wdrop: self.rnn = WeightDrop(self.rnn, wdrop)\n",
    "      \n",
    "        self.outp = nn.Linear(n_rnn_hidden * 2 * self.n_dirs, out_sz)\n",
    "        \n",
    "        # initialize weights\n",
    "        kaiming_normal(self.outp.weight.data)\n",
    "        \n",
    "        # init hidden\n",
    "        self.init_hidden(self.bsz)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        bsz = seq.size(1)\n",
    "        if (self.hidden[0].size(1) != bsz): self.init_hidden(bsz)\n",
    "        \n",
    "        x = self.emb_with_drop(seq, dropout=self.dropout_emb if self.training else 0)\n",
    "        x = self.dropout_after_emb(x)\n",
    "        \n",
    "        output, h = self.rnn(x, self.hidden)        \n",
    "        self.hidden = repackage_var(h)\n",
    "        \n",
    "        sl, bs, _ = output.size()\n",
    "  \n",
    "#         pdb.set_trace()\n",
    "        avg_pool = F.adaptive_avg_pool1d(output.permute(1,2,0), 1).view(bs,-1)   \n",
    "        max_pool = F.adaptive_max_pool1d(output.permute(1,2,0), 1).view(bs,-1) \n",
    "        \n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        outp = F.sigmoid(self.outp(x))\n",
    "        \n",
    "        return outp\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        self.hidden = (V(torch.zeros(self.n_dirs * self.n_rnn_layers, bsz, self.n_rnn_hidden)),\n",
    "                       V(torch.zeros(self.n_dirs * self.n_rnn_layers, bsz, self.n_rnn_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLstm(\n",
       "  (dropout_after_emb): LockedDropout(\n",
       "  )\n",
       "  (emb): Embedding(18777, 300)\n",
       "  (emb_with_drop): EmbeddingDropout(\n",
       "    (embed): Embedding(18777, 300)\n",
       "  )\n",
       "  (rnn): WeightDrop(\n",
       "    (module): LSTM(300, 128, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (outp): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sz = len(vocab.tokens)\n",
    "emb_sz = 300\n",
    "out_sz = 6\n",
    "\n",
    "n_rnn_hidden = 128\n",
    "n_rnn_layers = 1\n",
    "bi_dir = True\n",
    "\n",
    "model = SimpleLstm(vocab_sz, emb_sz, n_rnn_hidden, n_rnn_layers, True, out_sz, bsz=bsz)\n",
    "model#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = LayerOptimizer(optim.Adam, model, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, md, 1, lo.opt, F.binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, md, 1, lo.opt, F.binary_cross_entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
